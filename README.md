# Phenology
Разработка полносвязной нейронной сети на базе фреймворка TensorFlow Python для анализа влияния климатических факторов на фенологию растений
1. Архитектура нейронной сети
Тип модели: Полносвязная нейронная сеть
Слои:
    1.  Входной слой (input_layer): (INPUT_FEATURES=10). На вход подается 10 признаков (климатических факторов).
    2.  Скрытый слой 1 (hidden_1): Первый обучаемый слой Dense со 128 нейронами. Получает данные от входного слоя (10 признаков).
    3.  Слой регуляризации Dropout (0.3): Случайным образом "отключает" (обнуляет) 30% выходов нейронов предыдущего слоя во время обучения для борьбы с переобучением.
    4.  Скрытый слой 2 (hidden_2): Слой Dense с 64 нейронами.
    5.  Слой регуляризации Dropout (0.3).
    6.  Скрытый слой 3 (hidden_3): Слой Dense с 32 нейронами.
    7.  Слой регуляризации Dropout (0.2).
    8. Скрытый слой 4 (hidden_4): Слой Dense с 16 нейронами.
    9.  Слой регуляризации Dropout (0.2).
   10.  Выходной слой (output_layer): Слой Dense с 4 нейронами (OUTPUT_CLASSES). Каждый нейрон соответствует вероятности принадлежности входного образца к одному из 4-х классов фенофаз (покой, распускание почек, цветение, плодоношение).
Количество параметров:
1.	Первый скрытый слой
Входных признаков: INPUT_FEATURES = 10
Число нейронов: HIDDEN_LAYER_1 = 128
Веса: 10 * 128 = 1280
Смещения: 128
Итого: 1280 + 128 = 1408
2.	Второй скрытый слой
Входов: 128
Нейронов: 64
Веса: 128 * 64 = 8192
Смещения: 64
Итого: 8192 + 64 = 8256
3.	Третий скрытый слой
Входов: 64
Нейронов: 32
Веса: 64 * 32 = 2048
Смещения: 32
Итого: 2048 + 32 = 2080
4.	Четвёртый скрытый слой
Входов: 32
Нейронов: 16
Веса: 32 * 16 = 512
Смещения: 16
Итого: 512 + 16 = 528
5.	Выходной слой
Входов: 16
Нейронов: 4
Веса: 16 * 4 = 64
Смещения: 4
Итого: 64 + 4 = 68
Общее число параметров: 1408 + 8256 +2080 + 528 + 68 = 12340
Поток данных: 
Вектор из 10 признаков → Слой 1 (128 нейрона) → Dropout(0.3) → Слой 2 (64 нейрона) → Dropout(0.3) → Слой 3 (32 нейронов) → Dropout(0.2) → Слой 4 (16 нейронов) → Dropout(0.2) → Слой 5 (4 нейрона) → Вектор из 4 вероятностей.


2. Параметры и гиперпараметры
Гиперпараметры обучения:
EPOCHS =100
BATCH_SIZE = 64
VALIDATION_SPLIT = 0.15
TEST_SIZE = 0.15
LEARNING_RATE = 0.0001

Параметры архитектуры:
INPUT_FEATURES = 10: Задается входными данными.
HIDDEN_LAYER_1/2/3/4 = 128, 64, 32, 16: Количество нейронов в скрытых слоях. 
OUTPUT_CLASSES = 4: Задается постановкой задачи (4 фенофазы).
Коэффициенты Dropout: 0.3,0.3, 0.2, 0.2. 

Функции активации:
Скрытые слои: ReLU. Выбор обусловлен:
        1.  Вычислительная эффективность: Не требует сложных математических операций.
        2.  Устранение проблемы затухающих градиентов: В отличие от сигмоиды/тангенса, производная ReLu не стремится к нулю для положительных аргументов, что позволяет эффективно обучать глубокие сети.
Выходной слой Softmax. Выбор обусловлен:
1.	Преобразует выходы 4-х нейронов в вектор вероятностей, где сумма всех элементов равна 1. Это идеально для многоклассовой классификации, так как позволяет интерпретировать результат как вероятность принадлежности к каждому классу.

Оптимизатор: Adam. Автоматически настраивает скорость обучения для каждого параметра. 

Функция потерь (Loss): categorical_crossentropy. Это стандартная и математически обоснованная функция потерь для задач многоклассовой классификации, когда метки представлены в формате one-hot encoding. Она измеряет расхождение между распределением вероятностей, предсказанным сетью (softmax), и истинным распределением (one-hot вектор).

Визуализация:
    1.  Графики обучения: История изменения точности (accuracy) и функции потерь (loss) на тренировочном и валидационном наборах по эпохам. Позволяет отследить сходимость, переобучение (сильный рост val_loss при падении train_loss) или недообучение.
    2.  Матрица ошибок (Confusion Matrix): Таблица, показывающая, сколько примеров каждого класса были классифицированы правильно и в какие другие классы они были ошибочно отнесены. Визуализируется с помощью seaborn.heatmap.


3. Работа нейронной сети

Работу можно разделить на этапы:

1.  Инициализация: Веса и смещения всех слоев Dense инициализируются случайными малыми значениями.
2.  Прямое распространение (Forward Pass): Для каждого батча из 64 примеров:
    *   Данные проходят через все слои последовательно.
    *   На каждом Dense-слое: выход = activation(Вход * Веса + Смещение).
    *   На Dropout-слоях во время обучения часть значений обнуляется.
    *   На выходе получается вектор из 4-х чисел (логиты).
    *   Функция softmax преобразует логиты в вероятности.
3.  Вычисление ошибки: Сравнивается выходной вектор вероятностей (y_pred) с истинной one-hot меткой (y_true) с помощью функции categorical_crossentropy. Вычисляется средняя ошибка по всему батчу.
4.  Обратное распространение ошибки:
    *   Алгоритм вычисляет градиент функции потерь по всем весам и смещениям сети (каждый параметр вносит свой вклад в итоговую ошибку). Это делается с помощью цепного правила дифференцирования, начиная с выходного слоя.
    *   Оптимизатор Adam использует эти градиенты, чтобы адаптивно обновить все веса и смещения в сторону их уменьшения (шаг против градиента).
5.  Эпоха: Шаги 2-4 повторяются для всех батчей в тренировочном наборе. Это составляет одну эпоху.
6.  Валидация: После каждой эпохи сеть в режиме без Dropout и без обновления весов (model.evaluate) прогоняет валидационный набор (15% от X_train). Это дает независимую оценку качества.
7.  Цикл обучения: Шаги 2-6 повторяются для 100 эпох. История метрик сохраняется в объекте history.
8.  Тестирование: После окончания обучения сеть оценивается на полностью изолированном тестовом наборе (X_test, y_test), который не использовался ни для обучения, ни для валидации.


4. Анализ метрик нейросети

Основные метрики:
 Accuracy (Общая точность): Процент правильно классифицированных примеров. Легко интерпретируется, но может быть неинформативна при несбалансированных классах (test_accuracy)

Инструменты анализа в коде:
    1.  Графики обучения:
        - Сходимость: Кривые loss и val_loss стабилизируются на низком значении.
        -   Переобучение: train_loss продолжает падать, а val_loss начинает расти после определенной эпохи. В коде этому косвенно противодействует Dropout.
        - Недообучение: Обе кривые потерь (train и val) высокие и/или не снижаются.
        <img width="1749" height="911" alt="image" src="https://github.com/user-attachments/assets/6c3dc3d3-8184-4c5e-b0eb-5a1ecd9a1191" />
    2.  Матрица ошибок: Позволяет увидеть типичные ошибки модели. Например, часто ли она путает "Bud Break" и "Flowering". Диагональные элементы должны быть значительно выше недиагональных.
        <img width="845" height="961" alt="image" src="https://github.com/user-attachments/assets/d1486b73-e803-418d-a5f4-a801623fcc20" />
    3.  Тестирование на примерах: Позволяет провести качественный (интерпретируемый) анализ. Проверка на конкретных, логически понятных случаях (зима, весна, лето) дает уверенность в том, что сеть выучила осмысленные зависимости, а не просто запомнила данные. Высокая "уверенность" (вероятность) на таких примерах - хороший знак.
        <img width="978" height="359" alt="image" src="https://github.com/user-attachments/assets/ed52c09b-9acc-414d-95d7-35791d1ea562" />
