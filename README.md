# Phenology
Разработка полносвязной нейронной сети на базе фреймворка TensorFlow Python для анализа влияния климатических факторов на фенологию растений
1. Архитектура нейронной сети
Тип модели: Полносвязная нейронная сеть
Слои:
    1.  Входной слой (input_layer): Слой данных, определяющий форму входного вектора: (INPUT_FEATURES=10,). То есть на вход подается 10 признаков (климатических факторов).
    2.  Скрытый слой 1 (hidden_1): Первый обучаемый слой Dense с 64 нейронами. Получает данные от входного слоя (10 признаков).
    3.  Слой регуляризации Dropout (0.3): Случайным образом "отключает" (обнуляет) 30% выходов нейронов предыдущего слоя во время обучения для борьбы с переобучением.
    4.  Скрытый слой 2 (hidden_2): Слой Dense с 32 нейронами.
    5.  Слой регуляризации Dropout (0.2): "Отключает" 20% выходов из второго скрытого слоя.
    6.  Скрытый слой 3 (hidden_3): Слой Dense с 16 нейронами.
    7.  Слой регуляризации Dropout (0.2): Аналогично, "отключает" 20% выходов.
    8.  Выходной слой (output_layer): Слой Dense с 4 нейронами (OUTPUT_CLASSES). Каждый нейрон соответствует вероятности принадлежности входного образца к одному из 4-х классов фенофаз (покой, распускание почек, цветение, плодоношение).
Количество параметров:
Общее число параметров складывается из весов и смещений. Для первого слоя: (10 входов * 64 нейрона) + 64 смещения = 704. 
Аналогично для остальных. Сумма (указанная в model.summary()) составит 704 + 2080 + 1056 + 528 + 68 = 4436 обучаемых параметров.
Поток данных: 
Вектор из 10 признаков → Слой 1 (64 нейрона) → Dropout → Слой 2 (32 нейрона) → Dropout → Слой 3 (16 нейронов) → Dropout → Слой 4 (4 нейрона) → Вектор из 4 вероятностей.

2. Параметры и гиперпараметры
Гиперпараметры обучения:
EPOCHS = 200: Количество полных проходов всего тренировочного набора данных через сеть. Достаточно большое значение, чтобы сеть могла сойтись.
BATCH_SIZE = 32: Размер мини-пакета (батча). Столько примеров обрабатывается сетью перед одним обновлением весов. 32 - стандартный компромисс между скоростью (большие батчи) и качеством обобщения (меньшие батчи).
VALIDATION_SPLIT = 0.15: Доля тренировочных данных (15%), которая будет использована для валидации (проверки) в процессе обучения.
TEST_SIZE = 0.15: Доля всех данных (15%), выделенная для финального тестирования после обучения.
LEARNING_RATE = 0.001: Скорость обучения для оптимизатора Adam. Определяет размер шага при обновлении весов. 0.001 - стандартное, хорошо работающее значение для Adam.

Параметры архитектуры:
INPUT_FEATURES = 10: Задается входными данными.
HIDDEN_LAYER_1/2/3 = 64, 32, 16: Количество нейронов в скрытых слоях. Выбрана классическая нисходящая структура (64 -> 32 -> 16), которая позволяет сети сначала выделить много признаков, а затем постепенно скомбинировать их в более абстрактные для классификации.
OUTPUT_CLASSES = 4: Задается постановкой задачи (4 фенофазы).
Коэффициенты Dropout: 0.3, 0.2, 0.2. Более высокий dropout после первого слоя, где нейронов больше и риск переобучения выше.

Функции активации:
Скрытые слои: ReLU. Выбор обусловлен:
        1.  Вычислительная эффективность: Не требует сложных математических операций.
        2.  Устранение проблемы затухающих градиентов: В отличие от сигмоиды/тангенса, производная ReLu не стремится к нулю для положительных аргументов, что позволяет эффективно обучать глубокие сети.
Выходной слой Softmax. Выбор обусловлен:
1.	Преобразует выходы 4-х нейронов в вектор вероятностей, где сумма всех элементов равна 1. Это идеально для многоклассовой классификации, так как позволяет интерпретировать результат как вероятность принадлежности к каждому классу.

Оптимизатор: Adam. Автоматически настраивает скорость обучения для каждого параметра. 

Функция потерь (Loss): categorical_crossentropy. Это стандартная и математически обоснованная функция потерь для задач многоклассовой классификации, когда метки представлены в формате one-hot encoding. Она измеряет расхождение между распределением вероятностей, предсказанным сетью (softmax), и истинным распределением (one-hot вектор).

Визуализация:
    1.  Графики обучения: История изменения точности (accuracy) и функции потерь (loss) на тренировочном и валидационном наборах по эпохам. Позволяет отследить сходимость, переобучение (сильный рост val_loss при падении train_loss) или недообучение.
    2.  Матрица ошибок (Confusion Matrix): Таблица, показывающая, сколько примеров каждого класса были классифицированы правильно и в какие другие классы они были ошибочно отнесены. Визуализируется с помощью seaborn.heatmap.
    3.  Текстовый отчет о классификации: Выводит ключевые метрики (precision, recall, f1-score) для каждого класса и усредненные значения.

3. Работа нейронной сети

Работу можно разделить на этапы:

1.  Инициализация: Веса и смещения всех слоев Dense инициализируются случайными малыми значениями.
2.  Прямое распространение (Forward Pass): Для каждого батча из 32 примеров:
    *   Данные проходят через все слои последовательно.
    *   На каждом Dense-слое: выход = activation(Вход * Веса + Смещение).
    *   На Dropout-слоях во время обучения часть значений обнуляется.
    *   На выходе получается вектор из 4-х чисел (логиты).
    *   Функция softmax преобразует логиты в вероятности.
3.  Вычисление ошибки: Сравнивается выходной вектор вероятностей (y_pred) с истинной one-hot меткой (y_true) с помощью функции categorical_crossentropy. Вычисляется средняя ошибка по всему батчу.
4.  Обратное распространение ошибки:
    *   Алгоритм вычисляет градиент функции потерь по всем весам и смещениям сети (каждый параметр вносит свой вклад в итоговую ошибку). Это делается с помощью цепного правила дифференцирования, начиная с выходного слоя.
    *   Оптимизатор Adam использует эти градиенты, чтобы адаптивно обновить все веса и смещения в сторону их уменьшения (шаг против градиента).
5.  Эпоха: Шаги 2-4 повторяются для всех батчей в тренировочном наборе. Это составляет одну эпоху.
6.  Валидация: После каждой эпохи сеть в режиме без Dropout и без обновления весов (model.evaluate) прогоняет валидационный набор (15% от X_train). Это дает независимую оценку качества.
7.  Цикл обучения: Шаги 2-6 повторяются для 200 эпох. История метрик сохраняется в объекте history.
8.  Тестирование: После окончания обучения сеть оценивается на полностью изолированном тестовом наборе (X_test, y_test), который не использовался ни для обучения, ни для валидации.

4. Анализ метрик нейросети

Основные метрики (из classification_report):
 Accuracy (Общая точность): Процент правильно классифицированных примеров. Легко интерпретируется, но может быть неинформативна при несбалансированных классах (test_accuracy)
 Precision (Точность) для класса X: Доля правильно предсказанных классов X среди всех примеров, которые сеть отнесла к X. Высокий precision означает мало ложных срабатываний для этого класса.
Recall (Полнота) для класса X: Доля истинных X, которые были успешно обнаружены сетью. Высокий recall означает, что сеть пропускает мало объектов класса X.
 F1-Score: Гармоническое среднее между Precision и Recall. Удобная агрегированная метрика, особенно для несбалансированных данных.
    - Macro avg: Усреднение метрик по классам без учета их количества.
    - Weighted avg: Усреднение метрик по классам с учетом поддержки (количества примеров в каждом классе).

Инструменты анализа в коде:
    1.  Графики обучения:
        - Сходимость: Кривые loss и val_loss стабилизируются на низком значении.
        -   Переобучение: train_loss продолжает падать, а val_loss начинает расти после определенной эпохи. В коде этому косвенно противодействует Dropout.
        - Недообучение: Обе кривые потерь (train и val) высокие и/или не снижаются.
    2.  Матрица ошибок: Позволяет увидеть типичные ошибки модели. Например, часто ли она путает "Bud Break" и "Flowering". Диагональные элементы должны быть значительно выше недиагональных.
    3.  Отчет о классификации: Дает количественную оценку качества для каждого класса в отдельности. Можно увидеть, на каком классе модель работает хуже всего (низкие precision/recall).
    4.  Тестирование на примерах: Позволяет провести качественный (интерпретируемый) анализ. Проверка на конкретных, логически понятных случаях (зима, весна, лето) дает уверенность в том, что сеть выучила осмысленные зависимости, а не просто запомнила данные. Высокая "уверенность" (вероятность) на таких примерах - хороший знак.
<img width="1280" height="455" alt="image" src="https://github.com/user-attachments/assets/43ea8e67-3571-4097-92f6-4bcf9abe70ab" />
